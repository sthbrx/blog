<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Store Halfword Byte-Reverse Indexed - Performance</title><link href="http://sthbrx.github.io/" rel="alternate"></link><link href="https://sthbrx.github.io/feeds/performance.atom.xml" rel="self"></link><id>http://sthbrx.github.io/</id><updated>2018-08-15T14:22:00+10:00</updated><subtitle>A Power Technical Blog</subtitle><entry><title>Improving performance of Phoronix benchmarks on POWER9</title><link href="http://sthbrx.github.io/blog/2018/08/15/improving-performance-of-phoronix-benchmarks-on-power9/" rel="alternate"></link><published>2018-08-15T14:22:00+10:00</published><updated>2018-08-15T14:22:00+10:00</updated><author><name>Rashmica Gupta</name></author><id>tag:sthbrx.github.io,2018-08-15:/blog/2018/08/15/improving-performance-of-phoronix-benchmarks-on-power9/</id><content type="html">&lt;p&gt;Recently Phoronix ran a range of
&lt;a href="https://www.phoronix.com/scan.php?page=article&amp;amp;item=power9-talos-2&amp;amp;num=1"&gt;benchmarks&lt;/a&gt;
comparing the performance of our POWER9 processor against the Intel Xeon and AMD
EPYC processors. &lt;/p&gt;
&lt;p&gt;We did well in the Stockfish, LLVM Compilation, Zstd compression, and the
Tinymembench benchmarks. A few of my colleagues did a bit of investigating into
some the benchmarks where we didn't perform quite so well.&lt;/p&gt;
&lt;h3&gt;LBM / Parboil&lt;/h3&gt;
&lt;p&gt;The &lt;a href="http://impact.crhc.illinois.edu/parboil/parboil.aspx"&gt;Parboil benchmarks&lt;/a&gt; are a
collection of programs from various scientific and commercial fields that are
useful for examining the performance and development of different architectures
and tools.  In this round of benchmarks Phoronix used the lbm
&lt;a href="https://www.spec.org/cpu2006/Docs/470.lbm.html"&gt;benchmark&lt;/a&gt;: a fluid dynamics
simulation using the Lattice-Boltzmann Method.&lt;/p&gt;
&lt;p&gt;lbm is an iterative algorithm - the problem is broken down into discrete
time steps, and at each time step a bunch of calculations are done to
simulate the change in the system. Each time step relies on the results
of the previous one.&lt;/p&gt;
&lt;p&gt;The benchmark uses OpenMP to parallelise the workload, spreading the
calculations done in each time step across many CPUs. The number of
calculations scales with the resolution of the simulation.&lt;/p&gt;
&lt;p&gt;Unfortunately, the resolution (and therefore the work done in each time
step) is too small for modern CPUs with large numbers of SMT (simultaneous multi-threading) threads. OpenMP 
doesn't have enough work to parallelise and the system stays relatively idle. This
means the benchmark scales relatively poorly, and is definitely
not making use of the large POWER9 system&lt;/p&gt;
&lt;p&gt;Also this benchmark is compiled without any optimisation. Recompiling with -O3 improves the
   results 3.2x on POWER9.&lt;/p&gt;
&lt;h3&gt;x264 Video Encoding&lt;/h3&gt;
&lt;p&gt;x264 is a library that encodes videos into the H.264/MPEG-4 format. x264 encoding
requires a lot of integer kernels doing operations on image elements. The math
and vectorisation optimisations are quite complex, so Nick only had a quick look at
the basics. The systems and environments (e.g. gcc version 8.1 for Skylake, 8.0
for POWER9) are not completely apples to apples so for now patterns are more
important than the absolute results. Interestingly the output video files between
architectures are not the same, particularly with different asm routines and 
compiler options used, which makes it difficult to verify the correctness of any changes.&lt;/p&gt;
&lt;p&gt;All tests were run single threaded to avoid any SMT effects.&lt;/p&gt;
&lt;p&gt;With the default upstream build of x264, Skylake is significantly faster than POWER9 on this benchmark
(Skylake: 9.20 fps, POWER9: 3.39 fps). POWER9 contains some vectorised routines, so an
initial suspicion is that Skylake's larger vector size may be responsible for its higher throughput.&lt;/p&gt;
&lt;p&gt;Let's test our vector size suspicion by restricting
Skylake to SSE4.2 code (with 128 bit vectors, the same width as POWER9). This hardly
slows down the x86 CPU at all (Skylake: 8.37 fps, POWER9: 3.39 fps), which indicates it's
not taking much advantage of the larger vectors.&lt;/p&gt;
&lt;p&gt;So the next guess would be that x86 just has more and better optimized versions of costly
functions (in the version of x264 that Phoronix used there are only six powerpc specific
files compared with 21 x86 specific files). Without the time or expertise to dig into the
complex task of writing vector code, we'll see if the compiler can help, and turn
on autovectorisation (x264 compiles with -fno-tree-vectorize by default, which disables 
auto vectorization). Looking at a perf profile of the benchmark we can see
that one costly function, quant_4x4x4, is not autovectorised. With a small change to the
code, gcc does vectorise it, giving a slight speedup with the output file checksum unchanged
(Skylake: 9.20 fps, POWER9: 3.83 fps).&lt;/p&gt;
&lt;p&gt;We got a small improvement with the compiler, but it looks like we may have gains left on the
table with our vector code. If you're interested in looking into this, we do have some
&lt;a href="https://www.bountysource.com/teams/ibm/bounties"&gt;active bounties&lt;/a&gt; for x264 (lu-zero/x264).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;th&gt;Skylake&lt;/th&gt;
&lt;th&gt;POWER9&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original - AVX256&lt;/td&gt;
&lt;td&gt;9.20 fps&lt;/td&gt;
&lt;td&gt;3.39 fps&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Original - SSE4.2&lt;/td&gt;
&lt;td&gt;8.37 fps&lt;/td&gt;
&lt;td&gt;3.39 fps&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Autovectorisation enabled, quant_4x4x4 vectorised&lt;/td&gt;
&lt;td&gt;9.20 fps&lt;/td&gt;
&lt;td&gt;3.83 fps&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nick also investigated running this benchmark with SMT enabled and across multiple cores, and it looks like the code is
not scalable enough to feed 176 threads on a 44 core system. Disabling SMT in parallel runs
actually helped, but there was still idle time. That may be another thing to look at,
although it may not be such a problem for smaller systems.&lt;/p&gt;
&lt;h3&gt;Primesieve&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://primesieve.org/"&gt;Primesieve&lt;/a&gt; is a program and C/C++
library that generates all the prime numbers below a given number. It uses an
optimised &lt;a href="https://upload.wikimedia.org/wikipedia/commons/b/b9/Sieve_of_Eratosthenes_animation.gif"&gt;Sieve of Eratosthenes&lt;/a&gt;
implementation.&lt;/p&gt;
&lt;p&gt;The algorithm uses the L1 cache size as the sieve size for the core loop.  This
is an issue when we are running in SMT mode (aka more than one thread per core)
as all threads on a core share the same L1 cache and so will constantly be 
invalidating each others cache-lines. As you can see
in the table below, running the benchmark in single threaded mode is 30% faster
than in SMT4 mode!&lt;/p&gt;
&lt;p&gt;This means in SMT-4 mode the workload is about 4x too large for the L1 cache.  A
better sieve size to use would be the L1 cache size / number of
threads per core. Anton posted a &lt;a href="https://github.com/kimwalisch/primesieve/pull/54"&gt;pull request&lt;/a&gt; 
to update the sieve size.&lt;/p&gt;
&lt;p&gt;It is interesting that the best overall performance on POWER9 is with the patch applied and in
SMT2 mode:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SMT level&lt;/th&gt;
&lt;th&gt;baseline&lt;/th&gt;
&lt;th&gt;patched&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;14.728s&lt;/td&gt;
&lt;td&gt;14.899s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;15.362s&lt;/td&gt;
&lt;td&gt;14.040s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;19.489s&lt;/td&gt;
&lt;td&gt;17.458s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;LAME&lt;/h3&gt;
&lt;p&gt;Despite its name, a recursive acronym for "LAME Ain't an MP3 Encoder",
&lt;a href="http://lame.sourceforge.net/"&gt;LAME&lt;/a&gt; is indeed an MP3 encoder.&lt;/p&gt;
&lt;p&gt;Due to configure options &lt;a href="https://sourceforge.net/p/lame/mailman/message/36371506/"&gt;not being parsed correctly&lt;/a&gt; this
benchmark is built without any optimisation regardless of architecture. We see a
massive speedup by turning optimisations on, and a further 6-8% speedup by
enabling
&lt;a href="https://sourceforge.net/p/lame/mailman/message/36372005/"&gt;USE_FAST_LOG&lt;/a&gt; (which
is already enabled for Intel).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;LAME&lt;/th&gt;
&lt;th&gt;Duration&lt;/th&gt;
&lt;th&gt;Speedup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Default&lt;/td&gt;
&lt;td&gt;82.1s&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With optimisation flags&lt;/td&gt;
&lt;td&gt;16.3s&lt;/td&gt;
&lt;td&gt;5.0x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;With optimisation flags and USE_FAST_LOG set&lt;/td&gt;
&lt;td&gt;15.6s&lt;/td&gt;
&lt;td&gt;5.3x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more detail see Joel's
&lt;a href="https://shenki.github.io/LameMP3-on-Power9/"&gt;writeup&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;FLAC&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://xiph.org/flac/"&gt;FLAC&lt;/a&gt; is an alternative encoding format to
MP3. But unlike MP3 encoding it is lossless!  The benchmark here was encoding
audio files into the FLAC format. &lt;/p&gt;
&lt;p&gt;The key part of this workload is missing
vector support for POWER8 and POWER9. Anton and Amitay submitted this
&lt;a href="http://lists.xiph.org/pipermail/flac-dev/2018-July/006351.html"&gt;patch series&lt;/a&gt; that
adds in POWER specific vector instructions. It also fixes the configuration options
to correctly detect the POWER8 and POWER9 platforms. With this patch series we get see about a 3x
improvement in this benchmark.&lt;/p&gt;
&lt;h3&gt;OpenSSL&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.openssl.org/"&gt;OpenSSL&lt;/a&gt; is among other things a cryptographic library. The Phoronix benchmark
measures the number of RSA 4096 signs per second:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ openssl speed -multi &lt;span class="k"&gt;$(&lt;/span&gt;nproc&lt;span class="k"&gt;)&lt;/span&gt; rsa4096
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Phoronix used OpenSSL-1.1.0f, which is almost half as slow for this benchmark (on POWER9) than mainline OpenSSL.
Mainline OpenSSL has some powerpc multiplication and squaring assembly code which seems
to be responsible for most of this speedup.&lt;/p&gt;
&lt;p&gt;To see this for yourself, add these four powerpc specific commits on top of OpenSSL-1.1.0f:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/openssl/openssl/commit/b17ff188b17499e83ca3b9df0be47a2f513ac3c5"&gt;perlasm/ppc-xlate.pl: recognize .type directive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openssl/openssl/commit/0310becc82d240288a4ab5c6656c10c18cab4454"&gt;bn/asm/ppc-mont.pl: prepare for extension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openssl/openssl/commit/68f6d2a02c8cc30c5c737fc948b7cf023a234b47"&gt;bn/asm/ppc-mont.pl: add optimized multiplication and squaring subroutines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openssl/openssl/commit/80d27cdb84985c697f8fabb7649abf1f54714d13"&gt;ppccap.c: engage new multipplication and squaring subroutines&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following results were from a dual 16-core POWER9:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Version of OpenSSL&lt;/th&gt;
&lt;th&gt;Signs/s&lt;/th&gt;
&lt;th&gt;Speedup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1.1.0f&lt;/td&gt;
&lt;td&gt;1921&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.0f with 4 patches&lt;/td&gt;
&lt;td&gt;3353&lt;/td&gt;
&lt;td&gt;1.74x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1-pre1&lt;/td&gt;
&lt;td&gt;3383&lt;/td&gt;
&lt;td&gt;1.76x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;SciKit-Learn&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://scikit-learn.org/"&gt;SciKit-Learn&lt;/a&gt; is a bunch of python tools for data mining and
analysis (aka machine learning).&lt;/p&gt;
&lt;p&gt;Joel noticed that the benchmark spent 92% of the time in libblas. Libblas is a
very basic BLAS (basic linear algebra subprograms) library that python-numpy
uses to do vector and matrix operations.  The default libblas on Ubuntu is only
compiled with -O2. Compiling with -Ofast and using alternative BLAS's that have
powerpc optimisations (such as libatlas or libopenblas) we see big improvements
in this benchmark:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;BLAS used&lt;/th&gt;
&lt;th&gt;Duration&lt;/th&gt;
&lt;th&gt;Speedup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;libblas -O2&lt;/td&gt;
&lt;td&gt;64.2s&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;libblas -Ofast&lt;/td&gt;
&lt;td&gt;36.1s&lt;/td&gt;
&lt;td&gt;1.8x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;libatlas&lt;/td&gt;
&lt;td&gt;8.3s&lt;/td&gt;
&lt;td&gt;7.7x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;libopenblas&lt;/td&gt;
&lt;td&gt;4.2s&lt;/td&gt;
&lt;td&gt;15.3x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can read more details about this
&lt;a href="https://shenki.github.io/Scikit-Learn-on-Power9/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Blender&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; is a 3D graphics suite that supports image rendering,
animation, simulation and game creation. On the surface it appears that Blender
2.79b (the distro package version that Phoronix used by system/blender-1.0.2)
failed to use more than 15 threads, even when "-t 128" was added to the Blender
command line.&lt;/p&gt;
&lt;p&gt;It turns out that even though this benchmark was supposed to be run on CPUs only
(you can choose to render on CPUs or GPUs), the GPU file was always being used.
The GPU file is configured with a very large tile size (256x256) -
which is &lt;a href="https://docs.blender.org/manual/en/dev/render/cycles/settings/scene/render/performance.html#tiles"&gt;fine for
GPUs&lt;/a&gt;
but not great for CPUs. The image size (1280x720) to tile size ratio limits the
number of jobs created and therefore the number threads used.&lt;/p&gt;
&lt;p&gt;To obtain a realistic CPU measurement with more that 15 threads you can force
the use of the CPU file by overwriting the GPU file with the CPU one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ cp
~/.phoronix-test-suite/installed-tests/system/blender-1.0.2/benchmark/pabellon_barcelona/pavillon_barcelone_cpu.blend
~/.phoronix-test-suite/installed-tests/system/blender-1.0.2/benchmark/pabellon_barcelona/pavillon_barcelone_gpu.blend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see in the image below, now all of the cores are being utilised!
&lt;img alt="Blender with CPU Blend file" src="/images/phoronix/blender-88threads.png" title="Blender with CPU Blend file"&gt;&lt;/p&gt;
&lt;p&gt;Fortunately this has already been fixed in 
&lt;a href="https://openbenchmarking.org/test/pts/blender"&gt;pts/blender-1.1.1&lt;/a&gt;.
Thanks to the &lt;a href="https://github.com/phoronix-test-suite/test-profiles/issues/24"&gt;report&lt;/a&gt; by Daniel it
has also been fixed in &lt;a href="http://openbenchmarking.org/test/system/blender-1.1.0"&gt;system/blender-1.1.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pinning the pts/bender-1.0.2, Pabellon Barcelona, CPU-Only test to a single
22-core POWER9 chip (&lt;code&gt;sudo ppc64_cpu --cores-on=22&lt;/code&gt;) and two POWER9 chips
(&lt;code&gt;sudo ppc64_cpu --cores-on=44&lt;/code&gt;) show a huge speedup:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Benchmark&lt;/th&gt;
&lt;th&gt;Duration (deviation over 3 runs)&lt;/th&gt;
&lt;th&gt;Speedup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline (GPU blend file)&lt;/td&gt;
&lt;td&gt;1509.97s (0.30%)&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Single 22-core POWER9 chip (CPU blend file)&lt;/td&gt;
&lt;td&gt;458.64s (0.19%)&lt;/td&gt;
&lt;td&gt;3.29x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Two 22-core POWER9 chips (CPU blend file)&lt;/td&gt;
&lt;td&gt;241.33s (0.25%)&lt;/td&gt;
&lt;td&gt;6.25x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;tl;dr&lt;/h3&gt;
&lt;p&gt;Some of the benchmarks where we don't perform as well as Intel are where the
benchmark has inline assembly for x86 but uses generic C compiler generated
assembly for POWER9. We could probably benefit with some more powerpc optimsed functions.&lt;/p&gt;
&lt;p&gt;We also found a couple of things that should result in better performance for all three architectures,
not just POWER.&lt;/p&gt;
&lt;p&gt;A summary of the performance improvements we found:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Benchmark&lt;/th&gt;
&lt;th&gt;Approximate Improvement&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Parboil&lt;/td&gt;
&lt;td&gt;3x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x264&lt;/td&gt;
&lt;td&gt;1.1x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Primesieve&lt;/td&gt;
&lt;td&gt;1.1x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LAME&lt;/td&gt;
&lt;td&gt;5x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FLAC&lt;/td&gt;
&lt;td&gt;3x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenSSL&lt;/td&gt;
&lt;td&gt;2x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SciKit-Learn&lt;/td&gt;
&lt;td&gt;7-15x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Blender&lt;/td&gt;
&lt;td&gt;3x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is obviously room for more improvements, especially with the Primesieve and x264 benchmarks,
but it would be interesting to see a re-run of the Phoronix benchmarks with these changes. &lt;/p&gt;
&lt;p&gt;Thanks to Anton, Daniel, Joel and Nick for the analysis of the above benchmarks.&lt;/p&gt;</content><category term="Performance"></category><category term="performance"></category><category term="phoronix"></category><category term="benchmarks"></category></entry><entry><title>Get off my lawn: separating Docker workloads using cgroups</title><link href="http://sthbrx.github.io/blog/2016/07/27/get-off-my-lawn-separating-docker-workloads-using-cgroups/" rel="alternate"></link><published>2016-07-27T13:30:00+10:00</published><updated>2016-07-27T13:30:00+10:00</updated><author><name>Daniel Axtens</name></author><id>tag:sthbrx.github.io,2016-07-27:/blog/2016/07/27/get-off-my-lawn-separating-docker-workloads-using-cgroups/</id><content type="html">&lt;p&gt;On my team, we do two different things in our Continuous Integration setup: build/functional tests, and performance tests. Build tests simply test whether a project builds, and, if the project provides a functional test suite, that the tests pass. We do a lot of MySQL/MariaDB testing this way. The other type of testing we do is performance tests: we build a project and then run a set of benchmarks against it. Python is a good example here.&lt;/p&gt;
&lt;p&gt;Build tests want as much grunt as possible. Performance tests, on the other hand, want a stable, isolated environment. Initially, we set up Jenkins so that performance and build tests never ran at the same time. Builds would get the entire machine, and performance tests would never have to share with anyone.&lt;/p&gt;
&lt;p&gt;This, while simple and effective, has some downsides. In POWER land, our machines are quite beefy. For example, one of the boxes I use - an S822L - has 4 sockets, each with 4 cores. At SMT-8 (an 8 way split of each core) that gives us 4 x 4 x 8 = 128 threads. It seems wasteful to lock this entire machine - all 128 threads - just so as to isolate a single-threaded test.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;So, &lt;strong&gt;can we partition our machine so that we can be running two different sorts of processes in a sufficiently isolated way?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What counts as 'sufficiently isolated'? Well, my performance tests are CPU bound, so I want CPU isolation. I also want memory, and in particular memory bandwith to be isolated. I don't particularly care about IO isolation as my tests aren't IO heavy. Lastly, I have a couple of tests that are very multithreaded, so I'd like to have enough of a machine for those test results to be interesting.&lt;/p&gt;
&lt;p&gt;For CPU isolation we have CPU affinity. We can also do something similar with memory. On a POWER8 system, memory is connected to individual P8s, not to some central point. This is a 'Non-Uniform Memory Architecture' (NUMA) setup: the directly attached memory will be very fast for a processor to access, and memory attached to other processors will be slower to access. An accessible guide (with very helpful diagrams!) is &lt;a href="http://www.redbooks.ibm.com/redpapers/pdfs/redp5098.pdf"&gt;the relevant RedBook (PDF)&lt;/a&gt;, chapter 2.&lt;/p&gt;
&lt;p&gt;We could achieve the isolation we want by dividing up CPUs and NUMA nodes between the competing workloads. Fortunately, all of the hardware NUMA information is plumbed nicely into Linux. Each P8 socket gets a corresponding NUMA node. &lt;code&gt;lscpu&lt;/code&gt; will tell you what CPUs correspond to which NUMA nodes (although what it calls a CPU we would call a hardware thread). If you install &lt;code&gt;numactl&lt;/code&gt;, you can use &lt;code&gt;numactl -H&lt;/code&gt; to get even more details.&lt;/p&gt;
&lt;p&gt;In our case, the relevant &lt;code&gt;lscpu&lt;/code&gt; output is thus:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;NUMA node0 CPU(s):     0-31
NUMA node1 CPU(s):     96-127
NUMA node16 CPU(s):    32-63
NUMA node17 CPU(s):    64-95
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now all we have to do is find some way to tell Linux to restrict a group of processes to a particular NUMA node and the corresponding CPUs. How? Enter control groups, or &lt;code&gt;cgroups&lt;/code&gt; for short. Processes can be put into a cgroup, and then a cgroup controller can control the resouces allocated to the cgroup. Cgroups are hierarchical, and there are controllers for a number of different ways you could control a group of processes. Most helpfully for us, there's one called &lt;code&gt;cpuset&lt;/code&gt;, which can control CPU affinity, and restrict memory allocation to a NUMA node.&lt;/p&gt;
&lt;p&gt;We then just have to get the processes into the relevant cgroup. Fortunately, Docker is incredibly helpful for this!&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; Docker containers are put in the &lt;code&gt;docker&lt;/code&gt; cgroup. Each container gets it's own cgroup under the docker cgroup, and fortunately Docker deals well with the somewhat broken state of cpuset inheritance.&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; So it suffices to create a cpuset cgroup for docker, and allocate some resources to it, and Docker will do the rest. Here we'll allocate the last 3 sockets and NUMA nodes to Docker containers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cgcreate -g cpuset:docker
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;32&lt;/span&gt;-127 &amp;gt; /sys/fs/cgroup/cpuset/docker/cpuset.cpus
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;,16-17 &amp;gt; /sys/fs/cgroup/cpuset/docker/cpuset.mems
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &amp;gt; /sys/fs/cgroup/cpuset/docker/cpuset.mem_hardwall
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;mem_hardwall&lt;/code&gt; prevents memory allocations under docker from spilling over into the one remaining NUMA node.&lt;/p&gt;
&lt;p&gt;So, does this work? I created a container with sysbench and then ran the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;root@0d3f339d4181:/# sysbench --test&lt;span class="o"&gt;=&lt;/span&gt;cpu --num-threads&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; --max-requests&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10000000&lt;/span&gt; run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I've asked for 128 threads, but the cgroup only has CPUs/hwthreads 32-127 allocated. So If I run htop, I shouldn't see any load on CPUs 0-31. What do I actually see?&lt;/p&gt;
&lt;p&gt;&lt;img alt="htop screenshot, showing load only on CPUs 32-127" src="/images/dja/cgroup1.png"&gt;&lt;/p&gt;
&lt;p&gt;It works! Now, we create a cgroup for performance tests using the first socket and NUMA node:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cgcreate -g cpuset:perf-cgroup
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;-31 &amp;gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.cpus
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &amp;gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.mems
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &amp;gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.mem_hardwall
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Docker conveniently lets us put new containers under a different cgroup, which means we can simply do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dja@p88 ~&amp;gt; docker run -it --rm --cgroup-parent&lt;span class="o"&gt;=&lt;/span&gt;/perf-cgroup/ ppc64le/ubuntu bash
root@b037049f94de:/# &lt;span class="c1"&gt;# ... install sysbench&lt;/span&gt;
root@b037049f94de:/# sysbench --test&lt;span class="o"&gt;=&lt;/span&gt;cpu --num-threads&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; --max-requests&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10000000&lt;/span&gt; run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And the result?&lt;/p&gt;
&lt;p&gt;&lt;img alt="htop screenshot, showing load only on CPUs 0-31" src="/images/dja/cgroup2.png"&gt;&lt;/p&gt;
&lt;p&gt;It works! My benchmark results also suggest this is sufficient isolation, and the rest of the team is happy to have more build resources to play with.&lt;/p&gt;
&lt;p&gt;There are some boring loose ends to tie up: if a build job does anything outside of docker (like clone a git repo), that doesn't come under the docker cgroup, and we have to interact with systemd. Because systemd doesn't know about cpuset, this is &lt;em&gt;quite&lt;/em&gt; fiddly. We also want this in a systemd unit so it runs on start up, and we want some code to tear it down. But I'll spare you the gory details.&lt;/p&gt;
&lt;p&gt;In summary, cgroups are surprisingly powerful and simple to work with, especially in conjunction with Docker and NUMA on Power!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;It gets worse! Before the performance test starts, all the running build jobs must drain. If we have 8 Jenkins executors running on the box, and a performance test job is the next in the queue, we have to wait for 8 running jobs to clear. If they all started at different times and have different runtimes, we will inevitably spend a fair chunk of time with the machine at less than full utilisation while we're waiting.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;At least, on Ubuntu 16.04. I haven't tested if this is true anywhere else.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;I hear this is getting better. It is also why systemd hasn't done cpuset inheritance yet.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Performance"></category><category term="cgroups"></category><category term="numa"></category><category term="p8"></category></entry><entry><title>Docker: Just Stop Using AUFS</title><link href="http://sthbrx.github.io/blog/2015/10/30/docker-just-stop-using-aufs/" rel="alternate"></link><published>2015-10-30T13:30:00+11:00</published><updated>2015-10-30T13:30:00+11:00</updated><author><name>Daniel Axtens</name></author><id>tag:sthbrx.github.io,2015-10-30:/blog/2015/10/30/docker-just-stop-using-aufs/</id><content type="html">&lt;p&gt;Docker's default storage driver on most Ubuntu installs is AUFS.&lt;/p&gt;
&lt;p&gt;Don't use it. Use Overlay instead. Here's why.&lt;/p&gt;
&lt;p&gt;First, some background. I'm testing the performance of the basic LAMP
stack on POWER. (LAMP is Linux + Apache + MySQL/MariaDB + PHP, by the
way.) To do more reliable and repeatable tests, I do my builds and
tests in Docker containers. (See &lt;a href="/blog/2015/10/12/a-tale-of-two-dockers/"&gt;my previous post&lt;/a&gt; for more info.)&lt;/p&gt;
&lt;p&gt;Each test downloads the source of Apache, MariaDB and PHP, and builds
them. This should be quick: the POWER8 system I'm building on has 160
hardware threads and 128 GB of memory. But I was finding that it was
only just keeping pace with a 2 core Intel VM on BlueMix.&lt;/p&gt;
&lt;p&gt;Why? Well, my first point of call was to observe a compilation under
&lt;code&gt;top&lt;/code&gt;. The header is below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="top header, showing over 70 percent of CPU time spent in the kernel" src="/images/dja/aufs/top-bad.png"&gt;&lt;/p&gt;
&lt;p&gt;Over 70% of CPU time is spent in the kernel?! That's weird. Let's dig
deeper.&lt;/p&gt;
&lt;p&gt;My next port of call for analysis of CPU-bound workloads is
&lt;code&gt;perf&lt;/code&gt;. &lt;code&gt;perf top&lt;/code&gt; reports astounding quantities of time in
spin-locks:&lt;/p&gt;
&lt;p&gt;&lt;img alt="display from perf top, showing 80 percent of time in a spinlock" src="/images/dja/aufs/perf-top-spinlock.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;perf top -g&lt;/code&gt; gives us some more information: the time is in system
calls. &lt;code&gt;open()&lt;/code&gt; and &lt;code&gt;stat()&lt;/code&gt; are the key culprits, and we can see a
number of file system functions are in play in the call-chains of the
spinlocks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="display from perf top -g, showing syscalls and file ops" src="/images/dja/aufs/perf-top-syscalls.png"&gt;&lt;/p&gt;
&lt;p&gt;Why are open and stat slow? Well, I know that the files are on an AUFS
mount. (&lt;code&gt;docker info&lt;/code&gt; will tell you what you're using if you're not
sure.) So, being something of a kernel hacker, I set out to find out
why. This did not go well. AUFS isn't upstream, it's a separate patch
set. Distros have been trying to deprecate it for years. Indeed, RHEL
doesn't ship it. (To it's credit, Docker seems to be trying to move
away from it.)&lt;/p&gt;
&lt;p&gt;Wanting to avoid the minor nightmare that is an out-of-tree patchset,
I looked at other storage drivers for Docker. &lt;a href="https://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html"&gt;This presentation is particularly good.&lt;/a&gt;
My choices are pretty simple: AUFS, btrfs, device-mapper or
Overlay. Overlay was an obvious choice: it doesn't need me to set up
device mapper on a cloud VM, or reformat things as btrfs.&lt;/p&gt;
&lt;p&gt;It's also easy to set up on Ubuntu:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;export/save any docker containers you care about.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add &lt;code&gt;--storage-driver=overlay&lt;/code&gt; option to &lt;code&gt;DOCKER_OPTS&lt;/code&gt; in &lt;code&gt;/etc/default/docker&lt;/code&gt;, and restart docker (&lt;code&gt;service docker restart&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;import/load the containters you exported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;verify that things work, then clear away your old storage directory (&lt;code&gt;/var/lib/docker/aufs&lt;/code&gt;). &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having moved my base container across, I set off another build.&lt;/p&gt;
&lt;p&gt;The first thing I noticed is that images are much slower to create with Overlay. But once that finishes, and a compile starts, things run much better:&lt;/p&gt;
&lt;p&gt;&lt;img alt="top, showing close to zero system time, and around 90 percent user time" src="/images/dja/aufs/top-good.png"&gt;&lt;/p&gt;
&lt;p&gt;The compiles went from taking painfully long to astonishingly fast. Winning.&lt;/p&gt;
&lt;p&gt;So in conclusion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you use Docker for something that involves open()ing or stat()ing files&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you want your machine to do real work, rather than spin in spinlocks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you want to use code that's upstream and thus much better supported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you want something less disruptive than the btrfs or dm storage drivers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;...then drop AUFS and switch to Overlay today.&lt;/p&gt;</content><category term="Performance"></category><category term="aufs"></category><category term="overlay"></category><category term="performance"></category></entry></feed>